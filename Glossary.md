ゼロからつくるDeepLearning② 自然言語処理編の勉強メモ

# Chapter2: 単語の分散表現
シソーラスの作成はコストがかかるが、コーパスと共起行列を用いれば自動で作成することができる
* シソーラス：類義語辞典
* 代表的なシソーラスとしてはWordNet
* コーパス：自然言語処理の研究用に収集された大規模なテキストデータ
* 分布仮説：単語の意味は周囲の単語（コンテキスト）によって形成される
* コンテキスト：注目する単語の左右の単語
* ウィンドウサイズ：コンテキストとして利用する左右の単語数
* コンテキストを用いて与えられた文章から共起行列を定義することができる
* 共起行列からcos類似度を用いて単語同士の類似性を評価できる
* 単語の分散表現：”単語の意味”をベクトルとして表現する（”単語の意味”は文脈により異なる。意味としては類義語などがある）

# Chapter3: Word2Vec
* word2vecは2層のニューラルネットワークのモデル
* CBOW（continuous bag-of-words）モデル：コンテキストからターゲットを推測するモデル（=ターゲットの左右N単語から、ターゲットの単語を推測する）
* CBOWでは入力層から中間層への全結合層の重みが単語の分散表現となる

# Chapter4: Word2Vecの高速化
* 言語モデル：単語の並びに対して確率を与える(単語列の自然さを確率的に評価できる)
* 言語モデルを用いれば文章生成も可能

# Chapter5: RNN
* RNNには重みが2つ存在する
* 1つは入力xを出力に変換する重みWxであり、もう1つは1つ前の出力を次の出力に変換する重みWh
* hは隠れ状態（隠れベクトル）と呼ぶ
* BPTT(BackPropagationThroughTime)：RNNにおける逆誤差伝搬法の呼び名
* Truncated PBTT：学習時にナットワークの繋ががりを適当な長さで断ち切ること

# Chapter6: ゲート付きRNN
* RNNでは配消失もしくは傾配爆発が生じる
* 勾配爆発が生じるつのはWhを繰り返し乗算するため
* 勾配爆発への対策としては勾配クリッピングという手法を用いる
* 勾配消失への対策としてはゲート付きRNNを用いる
* ゲート付きRNNには様々なモデルがあり、代表はLSTMとGRU
